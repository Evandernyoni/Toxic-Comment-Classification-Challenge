{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MuNks:Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented three models for the Toxicity challenge.\n",
    "   + RNN\n",
    "   + Logistic Regression Model\n",
    "   + Gated Recurrent Unit (GRU)\n",
    "\n",
    "An ensemble was created from the results of these three models. The RNN is made up of 5 layers:\n",
    "\n",
    "<ol>    \n",
    "    <li> The first layer concatenated the <b>Fasttext</b> and <b>Glove</b> embeddings </li>\n",
    "    <li> Spatial Dropout 1D (0.5)</li>\n",
    "    <li> Bidrectional CuDNNLSTM with kernel size 40</li>\n",
    "    <li> Bidrectional CuDNNRU with kernel size 40</li>\n",
    "    <li> Concatenation of the last state, max pool, average pool and two features: \"unique words rate\" and \"rate of all-caps words\"</li>\n",
    "    <li> Output dense layer </li>\n",
    "</ol>\n",
    "\n",
    "The hyperparameters were as follows:\n",
    "\n",
    "   + Batch_size = 32 & 64\n",
    "   + Epochs = 3 & 5\n",
    "   + Max Length = 50\n",
    "   + Max Features = 100,000\n",
    " \n",
    " For the Logistic Regression the following parameters were used:\n",
    " \n",
    "   + Solver = sag\n",
    "   + Inverse of regularization strength, C = 0.1\n",
    "\n",
    " For GRU, the following parameters were:\n",
    " \n",
    "   + Max Features = 200,000\n",
    "   + Embedding dimension  = 300\n",
    "   + Max Length = 500\n",
    "   \n",
    "   Fasttext embedding has a dimension of 300 and so the embedding size parameter was kept at 300.\n",
    "   \n",
    " An ensemble of the three results were then created. This yielded an ROC-AUC score of <b>0.9866</b>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, CuDNNGRU\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(15)\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the train, test and embedding files.\n",
    "### Instead traing own word embeddinngs pretrain fasttext embeddings are used -\"EMBEDDING_FILE \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/home/ubuntu/tc_data/train_preprocessed.csv')\n",
    "test = pd.read_csv('/home/ubuntu/tc_data/test_preprocessed.csv')\n",
    "submission = pd.read_csv('/home/ubuntu/tc_data/sample_submission.csv')\n",
    "\n",
    "EMBEDDING_FILE = '/home/ubuntu/nana/crawl-300d-2M.vec'\n",
    "\n",
    "X_train = train[\"comment_text\"].fillna(\"fillna\").values\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test[\"comment_text\"].fillna(\"fillna\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the fasttext embedding matrix and embedding_index(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define evaluation metric \n",
    "#### Which the Area under the curve, for each epoch it returns the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model with pretrained embeddings, and used bidirectional LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x1 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x)\n",
    "    x2 = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    conc = concatenate([x1, x2])\n",
    "    avg_pool = GlobalAveragePooling1D()(conc)\n",
    "    max_pool = GlobalMaxPooling1D()(conc)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    x = Dense(64, activation='relu')(conc)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "\n",
    "batch_size = 50\n",
    "epochs = 4\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on test set and write to csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test, batch_size=1024)\n",
    "submission[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]] = y_pred\n",
    "submission.to_csv('submission_fasttext.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For Data Cleaning\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "from sklearn.feature_extraction import text as sklearn_text\n",
    "\n",
    "# For Feature Extraction\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For Model Building\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# For Model Evaluation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "print('### Import data ###')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    " \n",
    "labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train_text = train['comment_text'].fillna(' ')\n",
    "test_text = test['comment_text'].fillna(' ')\n",
    "\n",
    "train_text[:20]\n",
    "\n",
    "replacement = { \"aren't\" : \"are not\",\n",
    "                \"can't\" : \"cannot\",\n",
    "                \"couldn't\" : \"could not\",\n",
    "                \"didn't\" : \"did not\",\n",
    "                \"doesn't\" : \"does not\",\n",
    "                \"don't\" : \"do not\",\n",
    "                \"hadn't\" : \"had not\",\n",
    "                \"hasn't\" : \"has not\",\n",
    "                \"haven't\" : \"have not\",\n",
    "                \"he'd\" : \"he would\",\n",
    "                \"he'll\" : \"he will\",\n",
    "                \"he's\" : \"he is\",\n",
    "                \"i'd\" : \"I would\",\n",
    "                \"i'd\" : \"I had\",\n",
    "                \"i'll\" : \"I will\",\n",
    "                \"i'm\" : \"I am\",\n",
    "                \"isn't\" : \"is not\",\n",
    "                \"it's\" : \"it is\",\n",
    "                \"it'll\":\"it will\",\n",
    "                \"i've\" : \"I have\",\n",
    "                \"let's\" : \"let us\",\n",
    "                \"mightn't\" : \"might not\",\n",
    "                \"mustn't\" : \"must not\",\n",
    "                \"shan't\" : \"shall not\",\n",
    "                \"she'd\" : \"she would\",\n",
    "                \"she'll\" : \"she will\",\n",
    "                \"she's\" : \"she is\",\n",
    "                \"shouldn't\" : \"should not\",\n",
    "                \"that's\" : \"that is\",\n",
    "                \"there's\" : \"there is\",\n",
    "                \"they'd\" : \"they would\",\n",
    "                \"they'll\" : \"they will\",\n",
    "                \"they're\" : \"they are\",\n",
    "                \"they've\" : \"they have\",\n",
    "                \"we'd\" : \"we would\",\n",
    "                \"we're\" : \"we are\",\n",
    "                \"weren't\" : \"were not\",\n",
    "                \"we've\" : \"we have\",\n",
    "                \"what'll\" : \"what will\",\n",
    "                \"what're\" : \"what are\",\n",
    "                \"what's\" : \"what is\",\n",
    "                \"what've\" : \"what have\",\n",
    "                \"where's\" : \"where is\",\n",
    "                \"who'd\" : \"who would\",\n",
    "                \"who'll\" : \"who will\",\n",
    "                \"who're\" : \"who are\",\n",
    "                \"who's\" : \"who is\",\n",
    "                \"who've\" : \"who have\",\n",
    "                \"won't\" : \"will not\",\n",
    "                \"wouldn't\" : \"would not\",\n",
    "                \"you'd\" : \"you would\",\n",
    "                \"you'll\" : \"you will\",\n",
    "                \"you're\" : \"you are\",\n",
    "                \"you've\" : \"you have\",\n",
    "                \"'re\": \" are\",\n",
    "                \"wasn't\": \"was not\",\n",
    "                \"we'll\":\" will\",\n",
    "                \"didn't\": \"did not\"\n",
    "              }\n",
    "\n",
    "\n",
    "replacement.update({\"im\" : \"i am\", \"youre\" : \"you are\", \"ur\" : \"you are\",\n",
    "                    \"theyre\" : \"they are\", \"pls\" : \"please\", \"fk\" : \"fuck\"})\n",
    "print('\\n#### Data Cleaning ####')\n",
    "\n",
    "def replace_comment(comment):\n",
    "    comment=comment.lower()\n",
    "    \n",
    "    # Replace words like gooood to good\n",
    "    comment = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', comment)\n",
    "    \n",
    "    # Normalize common abbreviations\n",
    "    words=comment.split(' ')\n",
    "    words=[replacement[word] if word in replacement else word for word in words]\n",
    "\n",
    "    comment_repl=\" \".join(words)\n",
    "    return comment_repl\n",
    "\n",
    "# Lower the case and replace common abbreviation\n",
    "train_text = train_text.apply(lambda x: replace_comment(x))\n",
    "test_text = test_text.apply(lambda x: replace_comment(x))\n",
    "\n",
    "############################\n",
    "# DATA CLEANING\n",
    "############################\n",
    "\n",
    "# For checking Regexp: https://regex101.com/\n",
    "def standardize_text(datafile):\n",
    "    datafile = datafile.str.lower()\n",
    "    # Remove website link\n",
    "    datafile = datafile.str.replace(r\"http\\S+\", \"\")\n",
    "    datafile = datafile.str.replace(r\"https\\S+\", \"\")\n",
    "    datafile = datafile.str.replace(r\"http\", \"\")\n",
    "    datafile = datafile.str.replace(r\"https\", \"\")\n",
    "    # Remove name tag\n",
    "    datafile = datafile.str.replace(r\"@\\S+\", \"\")\n",
    "    # Remove time related text\n",
    "    datafile = datafile.str.replace(r'\\w{3}[+-][0-9]{1,2}\\:[0-9]{2}\\b', \"\") # e.g. UTC+09:00\n",
    "    datafile = datafile.str.replace(r'\\d{1,2}\\:\\d{2}\\:\\d{2}', \"\")            # e.g. 18:09:01\n",
    "    datafile = datafile.str.replace(r'\\d{1,2}\\:\\d{2}', \"\")                  # e.g. 18:09\n",
    "    # Remove date related text\n",
    "        # e.g. 11/12/19, 11-1-19, 1.12.19, 11/12/2019  \n",
    "    datafile = datafile.str.replace(r'\\d{1,2}(?:\\/|\\-|\\.)\\d{1,2}(?:\\/|\\-|\\.)\\d{2,4}', \"\")\n",
    "        # e.g. 11 dec, 2019   11 dec 2019   dec 11, 2019\n",
    "    datafile = datafile.str.replace(r\"([\\d]{1,2}\\s(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)|(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)\\s[\\d]{1,2})(\\s|\\,|\\,\\s|\\s\\,)[\\d]{2,4}\", \"\")\n",
    "        # e.g. 11 december, 2019   11 december 2019   december 11, 2019\n",
    "    datafile = datafile.str.replace(r\"[\\d]{1,2}\\s(january|february|march|april|may|june|july|august|september|october|november|december)(\\s|\\,|\\,\\s|\\s\\,)[\\d]{2,4}\", \"\")\n",
    "        # Remove line breaks\n",
    "    datafile = datafile.str.replace(\"\\r\",\" \")\n",
    "    datafile = datafile.str.replace(\"\\n\",\" \")\n",
    "    # Remove special characters\n",
    "    datafile = datafile.str.replace(r\"[^A-Za-z0-9(),.!?@\\`\\\"\\_ ]\", \"\")\n",
    "    datafile = datafile.str.replace(' \"\" ','')\n",
    "    # Remove phone number and IP address\n",
    "    datafile = datafile.str.replace(r'\\d{8,}', \"\")\n",
    "    datafile = datafile.str.replace(r'\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', \"\")\n",
    "    # Adjust common abbreviation\n",
    "    datafile = datafile.str.replace(r\" you re \", \" you are \")\n",
    "    datafile = datafile.str.replace(r\" we re \", \" we are \")\n",
    "    datafile = datafile.str.replace(r\" they re \", \" they are \")\n",
    "    datafile = datafile.str.replace(r\"@\", \"at\")\n",
    "    return datafile\n",
    "\n",
    "# Use regular expressions to clean up pour data.\n",
    "train_text = standardize_text(train_text)\n",
    "test_text = standardize_text(test_text)\n",
    "\n",
    "############################\n",
    "# STOP WORD REMOVAL\n",
    "############################\n",
    "nltk.download('stopwords')\n",
    "stopwords_list = nltk.corpus.stopwords.words('english') # stopwords from nltk\n",
    "\n",
    "# Exclude from stopwords: not, cannot\n",
    "stopwords_list_rev = list(filter(lambda x: x not in ('not','cannot'), stopwords_list)) \n",
    "stopwords_list_rev.sort()\n",
    "\n",
    "train_text = train_text.apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords_list_rev))\n",
    "test_text = test_text.apply(lambda x: \" \".join(x for x in x.split() if x not in stopwords_list_rev))\n",
    "\n",
    "train_text[:20]\n",
    "\n",
    "############################\n",
    "# CHECKING COMMON WORDS\n",
    "############################\n",
    "\n",
    "\"\"\" Revised method based on:\n",
    "    Ref 1: https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline\n",
    "    Ref 2: https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams\n",
    "\"\"\"\n",
    "\n",
    "# Word 1-gram\n",
    "word_vectorizer_1 = TfidfVectorizer(sublinear_tf=True,\n",
    "                                    strip_accents='unicode',\n",
    "                                    analyzer='word',\n",
    "                                    token_pattern=r'\\w{1,}',\n",
    "                                    stop_words='english',\n",
    "                                    ngram_range=(1, 1),\n",
    "                                    max_features=10000\n",
    "                                    )\n",
    "# Word 2-gram\n",
    "word_vectorizer_2 = TfidfVectorizer(sublinear_tf=True,\n",
    "                                    strip_accents='unicode',\n",
    "                                    analyzer='word',\n",
    "                                    token_pattern=r'\\w{1,}',\n",
    "                                    stop_words=stopwords_list_rev,\n",
    "                                    ngram_range=(2, 2),\n",
    "                                    max_features=5000\n",
    "                                    )\n",
    "# Char 2 to 6-gram\n",
    "char_vectorizer = TfidfVectorizer(  sublinear_tf = True,\n",
    "                                    strip_accents = 'unicode',\n",
    "                                    analyzer = 'char',\n",
    "                                    stop_words = 'english',\n",
    "                                    ngram_range = (2, 6),\n",
    "                                    max_features = 50000\n",
    "                                 )\n",
    "\n",
    "# Fit vectorizer by all text\n",
    "all_text = pd.concat([train_text, test_text])\n",
    "print('\\n### Vectorizer fitting ###')\n",
    "word_vectorizer_1.fit(all_text)\n",
    "word_vectorizer_2.fit(all_text)\n",
    "char_vectorizer.fit(all_text)\n",
    "\n",
    "# Transform dataset to document-term matrix\n",
    "print('\\n### DTM Transforming (train) ###')\n",
    "train_word_features1 = word_vectorizer_1.transform(train_text)\n",
    "train_word_features2 = word_vectorizer_2.transform(train_text)\n",
    "train_char_features  = char_vectorizer.transform(train_text)\n",
    "print('\\n### DTM Transforming (test) ###')\n",
    "test_word_features1 = word_vectorizer_1.transform(test_text)\n",
    "test_word_features2 = word_vectorizer_2.transform(test_text)\n",
    "test_char_features  = char_vectorizer.transform(test_text)\n",
    "\n",
    "# Merge features\n",
    "print('\\n### Merging Features Martix ###')\n",
    "train_features = hstack([train_word_features1, train_word_features2, train_char_features])\n",
    "test_features = hstack([test_word_features1, test_word_features2, test_char_features])\n",
    "\n",
    "############################\n",
    "# BASE MODELS\n",
    "############################\n",
    "\n",
    "\"\"\" model 1 : Simple Logistic Regression (Well Calibrated)\n",
    "    https://www.kaggle.com/tunguz/logistic-regression-with-words-and-char-n-grams\n",
    "    # 0.9792\n",
    "\"\"\" \n",
    "scores = []\n",
    "\n",
    "LR = pd.DataFrame.from_dict({'id': sample_submission['id']}).sort_values('id')\n",
    "\n",
    "print('\\n### Model 1: Simple Logistic Regression ###')\n",
    "for label in labels:\n",
    "    pred_model = LogisticRegression(C=0.1, solver='sag')\n",
    "    #AUC\n",
    "    score = np.mean(cross_val_score( pred_model, train_features, train[label], cv=3, scoring='roc_auc'))\n",
    "    scores.append(score)\n",
    "    print('For {}, AUC is {}.'.format(label, score))\n",
    "    \n",
    "    pred_model.fit(train_features, train[label])\n",
    "    LR[label] = pred_model.predict_proba(test_features)[:, 1]\n",
    "    \n",
    "print('\\nOverall CV score is {}'.format(np.mean(scores)))\n",
    "\n",
    "\n",
    "\n",
    "############################\n",
    "# DATA BLENDING\n",
    "############################\n",
    "# https://www.kaggle.com/reppic/lazy-ensembling-algorithm\n",
    "\n",
    "# Controls weights when combining predictions\n",
    "# 0: equal average of all inputs; 1: up to 50% of weight going to least correlated input\n",
    "DENSITY_COEFF = 0\n",
    "assert DENSITY_COEFF >= 0.0 and DENSITY_COEFF <= 1.0\n",
    "\n",
    "# When merging 2 files with corr > OVER_CORR_CUTOFF \n",
    "# the result's weight is the max instead of the sum of the merged files' weights\n",
    "OVER_CORR_CUTOFF = 0.98\n",
    "assert OVER_CORR_CUTOFF >= 0.0 and OVER_CORR_CUTOFF <= 1.0\n",
    "\n",
    "###############################################\n",
    "\n",
    "def load_submissions():\n",
    "    csv_files = {'sub1': 'submission1.csv',\n",
    "                 'sub4': 'submission4.csv',\n",
    "                 'sub5': 'submission5.csv',\n",
    "                 'sub7': 'submission7.csv'\n",
    "                }\n",
    "    frames = { f:pd.read_csv(f).sort_values('id') for f in csv_files.values() }\n",
    "    models = [ m for m in csv_files.keys() ]\n",
    "    data = dict(zip(models, frames.values()))\n",
    "    # Adding LR model to import models\n",
    "    data = dict(data,**{'LR': LR})\n",
    "    del frames\n",
    "    return data\n",
    "\n",
    "def get_corr_mat(frames, label):\n",
    "    c = pd.DataFrame()\n",
    "    for datafile, values in frames.items():\n",
    "        c[datafile] = values[label]\n",
    "    cor = c.corr()\n",
    "    \n",
    "    # Set the diagonal correlation to zero for merging\n",
    "    for index, name in enumerate(cor):\n",
    "        cor.iat[index,index] = 0.0\n",
    "    del c\n",
    "    return cor\n",
    "\n",
    "\n",
    "def highest_corr(mat):\n",
    "    n_cor = np.array(mat.values)\n",
    "    corr = np.max(n_cor)\n",
    "    idx = np.unravel_index(np.argmax(n_cor, axis=None), n_cor.shape)\n",
    "    f1 = mat.columns[idx[0]]\n",
    "    f2 = mat.columns[idx[1]]\n",
    "    return corr,f1,f2\n",
    "\n",
    "\n",
    "def get_merge_weights(m1,m2,densities):\n",
    "    d1 = densities[m1]\n",
    "    d2 = densities[m2]\n",
    "    d_tot = d1 + d2\n",
    "    weights1 = 0.5*DENSITY_COEFF + (d1/d_tot)*(1-DENSITY_COEFF)\n",
    "    weights2 = 0.5*DENSITY_COEFF + (d2/d_tot)*(1-DENSITY_COEFF)\n",
    "    return weights1, weights2\n",
    "\n",
    "\n",
    "def ensemble_col(label,frames,densities):\n",
    "    if len(frames) == 1:\n",
    "        model, value = frames.popitem() # Pop the last item\n",
    "        return value[label]\n",
    "    else:\n",
    "        corr_mat = get_corr_mat(frames, label)\n",
    "        \n",
    "        corr, merge1, merge2 = highest_corr(corr_mat)\n",
    "        w1,w2 = get_merge_weights(merge1,merge2,densities)\n",
    "        \n",
    "        comb_model = pd.DataFrame()\n",
    "        comb_model[label] = (frames[merge1][label]*w1) + (frames[merge2][label]*w2)\n",
    "    \n",
    "        comb_col = merge1 + '_' + merge2\n",
    "        frames[comb_col] = comb_model\n",
    "    \n",
    "        if corr >= OVER_CORR_CUTOFF:\n",
    "            print('\\t',merge1,merge2,'  (OVER CORR)')\n",
    "            densities[comb_col] = max(densities[merge1],densities[merge2])\n",
    "        else:\n",
    "            densities[comb_col] = densities[merge1] + densities[merge2]\n",
    "        \n",
    "        del frames[merge1]\n",
    "        del frames[merge2]\n",
    "        del densities[merge1]\n",
    "        del densities[merge2]\n",
    "        return ensemble_col(label, frames, densities)\n",
    "\n",
    "print('\\n#### Data Blending ####')\n",
    "\n",
    "final_submission = pd.DataFrame.from_dict({'id': sample_submission['id']}).sort_values('id')\n",
    "\n",
    "for label in labels:\n",
    "    frames = load_submissions()\n",
    "    densities = { k: 1.0 for k in frames.keys() }   # Pre-set density as 1 to all models\n",
    "    \n",
    "    print('\\n\\n # ', label)\n",
    "    final_submission[label] = ensemble_col(label, frames, densities)\n",
    "    \n",
    "############################\n",
    "# Output\n",
    "############################\n",
    "\n",
    "final_submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the relevant libraries\n",
    "##\n",
    "## https://www.kaggle.com/larryfreeman/toxic-comments-code-for-alexander-s-9872-model\n",
    "## Alexander Burmistrov Implementation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import unidecode as uc\n",
    "import string\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "import gc\n",
    "import time\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, LSTM,Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import CuDNNLSTM, CuDNNGRU\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras import optimizers\n",
    "from keras.layers import Lambda\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import KFold\n",
    "from unidecode import unidecode\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "eng_stopwords = set(stopwords.words(\"english\"))\n",
    "nltk.download('stopwords')\n",
    "\n",
    "## ROC-AUC Score Class\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.max_score = 0\n",
    "        self.not_better_count = 0\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=1)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n",
    "            if (score > self.max_score):\n",
    "                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n",
    "                model.save_weights(\"best_weights.h5\")\n",
    "                self.max_score=score\n",
    "                self.not_better_count = 0\n",
    "            else:\n",
    "                self.not_better_count += 1\n",
    "                if self.not_better_count > 3:\n",
    "                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n",
    "                    self.model.stop_training = True\n",
    "\n",
    "\n",
    "## RNN Model Layers\n",
    "def getModel(features,clipvalue=1.,num_filters=40,dropout=0.5,embed_size=501):\n",
    "    \n",
    "    features_input = Input(shape=(features.shape[1],))\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    \n",
    "    # Layer 1: concatenated fasttext and glove twitter embeddings.\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    \n",
    "    # Uncomment for best result\n",
    "    # Layer 2: SpatialDropout1D(0.5)\n",
    "    x = SpatialDropout1D(dropout)(x)\n",
    "    \n",
    "    # Uncomment for best result\n",
    "    # Layer 3: Bidirectional CuDNNLSTM\n",
    "    x = Bidirectional(LSTM(num_filters, return_sequences=True))(x)\n",
    "\n",
    "\n",
    "    # Layer 4: Bidirectional CuDNNGRU\n",
    "    x, x_h, x_c = Bidirectional(GRU(num_filters, return_sequences=True, return_state = True))(x)  \n",
    "    \n",
    "    # Layer 5: A concatenation of the last state, maximum pool, average pool and \n",
    "    # two features: \"Unique words rate\" and \"Rate of all-caps words\"\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    \n",
    "    x = concatenate([avg_pool, x_h, max_pool,features_input])\n",
    "    \n",
    "    # Layer 6: output dense layer.\n",
    "    outp = Dense(6, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=[inp,features_input], outputs=outp)\n",
    "    adam = optimizers.adam(clipvalue=clipvalue)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "## Get data and start preprocessing here\n",
    "def getData():\n",
    "    \n",
    "    dataTrain = pd.read_csv('train.csv')\n",
    "    dataTest = pd.read_csv('test.csv')\n",
    "    \n",
    "    return dataTrain,dataTest\n",
    "\n",
    "\n",
    "special_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\n",
    "\n",
    "\n",
    "def cleanText(x):\n",
    "    x_ascii = unidecode(x)\n",
    "    x_clean = special_character_removal.sub('',x_ascii)\n",
    "    return x_clean\n",
    "\n",
    "\n",
    "\n",
    "## Add features\n",
    "def addFeatures(df):\n",
    "    \n",
    "    df['comment_text'] = df['comment_text'].apply(lambda x:str(x))\n",
    "    df['total_length'] = df['comment_text'].apply(len)\n",
    "    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n",
    "    df['caps_vs_length'] = df.apply(lambda row: float(row['capitals'])/float(row['total_length']),\n",
    "                                axis=1)\n",
    "    df['num_words'] = df.comment_text.str.count('\\S+')\n",
    "    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Get the data set\n",
    "train,test = getData()\n",
    "\n",
    "\n",
    "# Create a new column with cleaned text of the \"comment_text\" column\n",
    "train['clean_text'] = train['comment_text'].apply(lambda x: cleanText(str(x)))\n",
    "test['clean_text'] = test['comment_text'].apply(lambda x: cleanText(str(x)))\n",
    "\n",
    "\n",
    "# FillNAs in the \"clean_text\" column with \"Something\" and obtain the X_Train and Y_Train\n",
    "X_train = train['clean_text'].fillna(\"something\").values\n",
    "Y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "X_test = test['clean_text'].fillna(\"something\").values\n",
    "\n",
    "\n",
    "\n",
    "## Execute Add features with all\n",
    "train = addFeatures(train)\n",
    "test = addFeatures(test)\n",
    "\n",
    "## FillNAs in caps_vs_length & words_vs_unique with 0s\n",
    "features = train[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "test_features = test[['caps_vs_length', 'words_vs_unique']].fillna(0)\n",
    "\n",
    "# Transform\n",
    "ss = StandardScaler()\n",
    "ss.fit(np.vstack((features, test_features)))\n",
    "features = ss.transform(features)\n",
    "test_features = ss.transform(test_features)\n",
    "\n",
    "\n",
    "# For best score (Public: 9869, Private: 9865), change to max_features = 283759, maxlen = 900\n",
    "max_features = 10000\n",
    "maxlen = 50\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(X_train) + list(X_test))\n",
    "X_train_sequence = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequence = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "x_train = sequence.pad_sequences(X_train_sequence, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(X_test_sequence, maxlen=maxlen)\n",
    "print(len(tokenizer.word_index))\n",
    "\n",
    "\n",
    "# Load the FastText Web Crawl vectors\n",
    "# T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, A. Joulin. Advances in Pre-Training Distributed Word Representations\n",
    "#\n",
    "#\n",
    "EMBEDDING_FILE_FASTTEXT = \"/Users/nana/Documents/DataScience/ModuleIII/crawl-300d-2M.vec\"\n",
    "EMBEDDING_FILE_TWITTER=\"/Users/nana/Documents/DataScience/ModuleIII/glove/glove.twitter.27B.200d.txt\"\n",
    "\n",
    "def getCoeffs(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index_ft = dict(getCoeffs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_FASTTEXT,encoding='utf-8'))\n",
    "embeddings_index_tw = dict(getCoeffs(*o.strip().split()) for o in open(EMBEDDING_FILE_TWITTER,encoding='utf-8'))\n",
    "\n",
    "\n",
    "## Use the GENSIM library --> Word2vec embedding \n",
    "spell_model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE_FASTTEXT)\n",
    "\n",
    "\n",
    "# This code is  based on: Spellchecker using Word2vec by CPMP\n",
    "# https://www.kaggle.com/cpmpml/spell-checker-using-word2vec\n",
    "\n",
    "words = spell_model.index2word\n",
    "\n",
    "w_rank = {}\n",
    "for i,word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "\n",
    "WORDS = w_rank\n",
    "\n",
    "# Use fast text as vocabulary\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word): \n",
    "    \"Probability of `word`.\"\n",
    "    # use inverse of rank as proxy\n",
    "    # returns 0 if the word isn't in the dictionary\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "def singlify(word):\n",
    "    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\n",
    "\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words,501))\n",
    "\n",
    "something_tw = embeddings_index_tw.get(\"something\")\n",
    "something_ft = embeddings_index_ft.get(\"something\")\n",
    "\n",
    "something = np.zeros((501,))\n",
    "something[:300,] = something_ft\n",
    "something[300:500,] = something_tw\n",
    "something[500,] = 0\n",
    "\n",
    "def all_caps(word):\n",
    "    return len(word) > 1 and word.isupper()\n",
    "\n",
    "def embed_word(embedding_matrix,i,word):\n",
    "    embedding_vector_ft = embeddings_index_ft.get(word)\n",
    "    if embedding_vector_ft is not None: \n",
    "        if all_caps(word):\n",
    "            last_value = np.array([1])\n",
    "        else:\n",
    "            last_value = np.array([0])\n",
    "        embedding_matrix[i,:300] = embedding_vector_ft\n",
    "        embedding_matrix[i,500] = last_value\n",
    "        embedding_vector_tw = embeddings_index_tw.get(word)\n",
    "        if embedding_vector_tw is not None:\n",
    "            embedding_matrix[i,300:500] = embedding_vector_tw\n",
    "\n",
    "            \n",
    "# Fasttext vector is used by itself if there is no glove vector but not the other way around.\n",
    "for word, i in word_index.items():\n",
    "    \n",
    "    if i >= max_features: continue\n",
    "        \n",
    "    if embeddings_index_ft.get(word) is not None:\n",
    "        embed_word(embedding_matrix,i,word)\n",
    "    else:\n",
    "        # change to > 20 for better score.\n",
    "        if len(word) > 0:\n",
    "            embedding_matrix[i] = something\n",
    "        else:\n",
    "            word2 = correction(word)\n",
    "            if embeddings_index_ft.get(word2) is not None:\n",
    "                embed_word(embedding_matrix,i,word2)\n",
    "            else:\n",
    "                word2 = correction(singlify(word))\n",
    "                if embeddings_index_ft.get(word2) is not None:\n",
    "                    embed_word(embedding_matrix,i,word2)\n",
    "                else:\n",
    "                    embedding_matrix[i] = something\n",
    "                    \n",
    "                    \n",
    "\n",
    "##\n",
    "##   Begin writing the model here\n",
    "##\n",
    "model = getModel(features)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Used epochs=100 with early exiting for best score.\n",
    "epochs = 1\n",
    "gc.collect()\n",
    "K.clear_session()\n",
    "\n",
    "# Change to 10\n",
    "num_folds = 2 #number of folds\n",
    "\n",
    "predict = np.zeros((test.shape[0],6))\n",
    "\n",
    "# Uncomment for out-of-fold predictions\n",
    "#scores = []\n",
    "#oof_predict = np.zeros((train.shape[0],6))\n",
    "\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=239)\n",
    "\n",
    "for train_index, test_index in kf.split(x_train):\n",
    "    \n",
    "    kfold_y_train,kfold_y_test = Y_train[train_index], Y_train[test_index]\n",
    "    kfold_X_train = x_train[train_index]\n",
    "    kfold_X_features = features[train_index]\n",
    "    kfold_X_valid = x_train[test_index]\n",
    "    kfold_X_valid_features = features[test_index] \n",
    "    \n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    \n",
    "    model = getModel(features)\n",
    "    \n",
    "    ra_val = RocAucEvaluation(validation_data=([kfold_X_valid,kfold_X_valid_features], kfold_y_test), interval = 1)\n",
    "    \n",
    "    model.fit([kfold_X_train,kfold_X_features], kfold_y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n",
    "             callbacks = [ra_val])\n",
    "    gc.collect()\n",
    "    \n",
    "    #model.load_weights(bst_model_path)\n",
    "    model.load_weights(\"best_weights.h5\")\n",
    "    \n",
    "    predict += model.predict([x_test,test_features], batch_size=batch_size,verbose=1) / num_folds\n",
    "    \n",
    "    #gc.collect()\n",
    "    # uncomment for out of fold predictions\n",
    "    #oof_predict[test_index] = model.predict([kfold_X_valid, kfold_X_valid_features],batch_size=batch_size, verbose=1)\n",
    "    #cv_score = roc_auc_score(kfold_y_test, oof_predict[test_index])\n",
    "    \n",
    "    #scores.append(cv_score)\n",
    "    #print('score: ',cv_score)\n",
    "\n",
    "print(\"Done\")\n",
    "#print('Total CV score is {}'.format(np.mean(scores)))    \n",
    "\n",
    "\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "class_names = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "sample_submission[class_names] = predict\n",
    "sample_submission.to_csv('/Users/nana/Documents/DataScience/ModuleIII/munks_rnn_5ep_submission.csv',index=False)                  \n",
    "                    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
